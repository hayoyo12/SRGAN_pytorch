{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Improved_ESRGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxpHaFcukWFw"
      },
      "source": [
        "# **SRGAN**:\n",
        " high resolution이미지에서 low resolution으로 만들어서 super resolution으로 이미지를 복원 시키는 모델이라고 생각하시면 됩니다.\n",
        "\n",
        "참고 링크:\n",
        "\n",
        "1) https://github.com/kunalrdeshmukh/SRGAN/blob/master/SRGAN.ipynb\n",
        "\n",
        "2) https://www.kaggle.com/balraj98single-image-super-resolution-gan-srgan-pytorch\n",
        "\n",
        "3) https://github.com/leftthomas/SRGAN/blob/master/data_utils.py\n",
        "\n",
        "4) https://github.com/deepak112/Keras-SRGAN\n",
        "\n",
        "# **ESRGAN:**\n",
        "\n",
        "1) https://blog.naver.com/leeth5225/221645820290\n",
        "\n",
        "2) https://github.com/xinntao/ESRGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj1rh4_9o5K0"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, math, sys\n",
        "import glob, itertools\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchvision.models import vgg19\n",
        "from torchvision.datasets import CIFAR100 # household furniture dataset을 학습에 활용함\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.datasets as dset\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "from PIL import Image\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "random.seed(42)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b13VH1pr6Tt"
      },
      "source": [
        "Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-02ZJjhsquFl"
      },
      "source": [
        "# number of epochs of training\n",
        "n_epochs = 200\n",
        "\n",
        "# size of the batches\n",
        "batch_size = 4 # train\n",
        "# batch_size = 1 # test \n",
        "\n",
        "# adam: learning rate\n",
        "lr = 0.0002\n",
        "# adam: decay of first order momentum of gradient\n",
        "b1 = 0.9\n",
        "# adam: decay of second order momentum of gradient\n",
        "b2 = 0.999\n",
        "# number of cpu threads to use during batch generation = number of workers\n",
        "n_cpu = 2\n",
        "\n",
        "# high res. image height\n",
        "hr_height = 512\n",
        "# high res. image width\n",
        "hr_width = 512\n",
        "\n",
        "# # high res. image height # out of cuda memory\n",
        "# hr_height = 1024\n",
        "# # high res. image width\n",
        "# hr_width = 1024\n",
        "\n",
        "# #cifar-100은 32x32\n",
        "# hr_height = 32\n",
        "# # high res. image width\n",
        "# hr_width = 32\n",
        "\n",
        "# number of image channels = rgb\n",
        "channels = 3\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "hr_shape = (hr_height, hr_width)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjlmLjn-r96y"
      },
      "source": [
        "# **Dataset**\n",
        "\n",
        "아마도 coco train 2017의 chair & sofa를 사용하는게 좋을 것 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpZdVMDT8Yt6",
        "outputId": "91319728-25ed-40c9-a2c4-d7d34f8be5c2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f4nH9j90GwP"
      },
      "source": [
        "# Normalization parameters for pre-trained PyTorch models\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, files, hr_shape):\n",
        "        hr_height, hr_width = hr_shape\n",
        "        # Transforms for low resolution images and high resolution images\n",
        "        # low : high = 4 배\n",
        "        self.lr_transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((hr_height // 4, hr_height // 4), Image.BICUBIC),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std),\n",
        "            ]\n",
        "        )\n",
        "        self.hr_transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((hr_height, hr_height), Image.BICUBIC),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std),\n",
        "            ]\n",
        "        )\n",
        "        self.files = files\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.files[index % len(self.files)])\n",
        "        img_lr = self.lr_transform(img)\n",
        "        img_hr = self.hr_transform(img)\n",
        "\n",
        "        return {\"lr\": img_lr, \"hr\": img_hr}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "# denormalize\n",
        "def denormalize(tensors):\n",
        "  \"\"\" Denormalizes image tensors using mean and std \"\"\"\n",
        "  for c in range(3):\n",
        "      tensors[:, c].mul_(std[c]).add_(mean[c])\n",
        "  return torch.clamp(tensors, 0, 255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6iqzAmBxRpP"
      },
      "source": [
        "## **imaterialist**-furniture\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqsAxSVQwnJS"
      },
      "source": [
        "사무실 의자, 나무의자, 소파, 세면대\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH0jZkfWws5L"
      },
      "source": [
        "dataset_path = \"/content/drive/MyDrive/AI604_TeamProject/final_train_data\"\n",
        "\n",
        "train_dataloader = DataLoader(ImageDataset(glob.glob(dataset_path + \"/*.*\"), hr_shape=hr_shape), batch_size=batch_size, shuffle=True, num_workers=n_cpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llGSPT32Hjc5"
      },
      "source": [
        "# **Improved ESRGAN** Model Define"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUH8otfZLPdA"
      },
      "source": [
        "#VGG19를 사용한 Fixed Feature Extraction\n",
        "# activation이전을 불러와서 화면 밝기를 일정하게 만들어 준다\n",
        "class FeatureExtractor(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "\n",
        "    vgg19_model = vgg19(pretrained=True)\n",
        "    # self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:35]).eval()\n",
        "    self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:35])\n",
        "\n",
        "    # for param in self.feature_extractor.parameters():\n",
        "    #   param.requires_grad = False\n",
        "\n",
        "  def forward(self, img):\n",
        "    return self.feature_extractor(img)\n",
        "\n",
        "# 5 layers로 바꿈 & BN 제거\n",
        "class ResnetBlock(nn.Module):\n",
        "  def __init__(self, filters, res_scale=0.2):\n",
        "    super(ResnetBlock, self).__init__()\n",
        "    self.res_scale = res_scale\n",
        "         \n",
        "    def block(in_features, non_linearity=True):\n",
        "      layers = [nn.Conv2d(in_features, filters, 3, 1, 1, bias=True)]\n",
        "      if non_linearity:\n",
        "          layers += [nn.LeakyReLU(negative_slope=0.2, inplace=True)]\n",
        "      return nn.Sequential(*layers)\n",
        "\n",
        "    self.conv1 = block(in_features=1 * filters)\n",
        "    self.conv2 = block(in_features=2 * filters)\n",
        "    self.conv3 = block(in_features=3 * filters)\n",
        "    self.conv4 = block(in_features=4 * filters)\n",
        "    self.conv5 = block(in_features=5 * filters, non_linearity=False)\n",
        "\n",
        "    self.blocks = [self.conv1, self.conv2, self.conv3, self.conv4, self.conv5]\n",
        "\n",
        "  def forward(self, x):\n",
        "    inputs = x\n",
        "    for block in self.blocks:\n",
        "        out = block(inputs)\n",
        "        inputs = torch.cat([inputs, out], 1)\n",
        "    return out.mul(self.res_scale) + x\n",
        "\n",
        "class RRDB(nn.Module):\n",
        "    def __init__(self, filters, res_scale=0.2):\n",
        "      super(RRDB, self).__init__()\n",
        "      self.res_scale = res_scale\n",
        "      self.dense_blocks = nn.Sequential(\n",
        "          ResnetBlock(filters), ResnetBlock(filters), ResnetBlock(filters))\n",
        "\n",
        "    def forward(self, x):\n",
        "      return self.dense_blocks(x) * self.res_scale + x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9dn63hYuL9U"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, input_shape):\n",
        "    super(Discriminator, self).__init__()\n",
        "\n",
        "    self.input_shape = input_shape\n",
        "    in_channels, in_height, in_width = self.input_shape\n",
        "    patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n",
        "    self.output_shape = (1, patch_h, patch_w)\n",
        "\n",
        "    def discriminator_block(in_filters, out_filters, first_block=False):\n",
        "      layers = []\n",
        "      layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1, bias = False))\n",
        "      if not first_block:\n",
        "          layers.append(nn.BatchNorm2d(out_filters))\n",
        "      layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "      # layers.append(nn.Dropout2d(0.2))\n",
        "      layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1, bias =False))\n",
        "      layers.append(nn.BatchNorm2d(out_filters))\n",
        "      layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "      # layers.append(nn.Dropout2d(0.25))\n",
        "      return layers\n",
        "\n",
        "    layers = []\n",
        "    in_filters = in_channels\n",
        "    for i, out_filters in enumerate([64, 128, 256, 512]):\n",
        "      layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n",
        "      in_filters = out_filters\n",
        "\n",
        "    layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n",
        "    # layers.append(nn.Dropout2d(0.3))# add dropout to prevent discriminator overfitting--너무 안좋아져서 제거\n",
        "\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, img):\n",
        "    return self.model(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNrAJ-KtH58C"
      },
      "source": [
        "# ESRGAN처럼 Generator에서 BN을 없애고 RRDB로 바꿔준다\n",
        "# 더 많은 layer와 block은 성능을 좋게 해주고 복잡성과 메모리 사용 감소\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, in_channels=3, out_channels=3, filters = 64, n_residual_blocks=23):\n",
        "    super(Generator, self).__init__()\n",
        "\n",
        "    # First layer\n",
        "    self.conv1 = nn.Conv2d(in_channels, filters, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    # Residual blocks\n",
        "    res_blocks = []\n",
        "    for i in range(n_residual_blocks):\n",
        "        res_blocks.append(ResnetBlock(filters))\n",
        "    self.res_blocks = nn.Sequential(*res_blocks)\n",
        "\n",
        "    # Second conv layer post residual blocks\n",
        "    self.conv2 = nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    # Upsampling layers\n",
        "    upsampling = []\n",
        "    for out_features in range(2):\n",
        "        upsampling += [\n",
        "            # nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(filters, filters*4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.PixelShuffle(upscale_factor=2),\n",
        "        ]\n",
        "    self.upsampling = nn.Sequential(*upsampling)\n",
        "\n",
        "    # Final output layer\n",
        "    self.conv3 = nn.Sequential(\n",
        "        nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Conv2d(filters, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    out1 = self.conv1(x)\n",
        "    out = self.res_blocks(out1)\n",
        "    out2 = self.conv2(out)\n",
        "    out = torch.add(out1, out2)\n",
        "    out = self.upsampling(out)\n",
        "    out = self.conv3(out)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9qjr-1KXX8m",
        "outputId": "f998818a-5f36-44bb-ecb5-b14d5530c3e0"
      },
      "source": [
        "netG = Generator(channels, filters = 64, n_residual_blocks = 23).cuda()\n",
        "netD = Discriminator(input_shape=(channels, *hr_shape)).cuda()\n",
        "feature_extractor = FeatureExtractor().cuda()\n",
        "\n",
        "feature_extractor.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FeatureExtractor(\n",
              "  (feature_extractor): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (24): ReLU(inplace=True)\n",
              "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (26): ReLU(inplace=True)\n",
              "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): ReLU(inplace=True)\n",
              "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (33): ReLU(inplace=True)\n",
              "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P6SvahFIFjD"
      },
      "source": [
        "## **Loss Function** and **Optimizer**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BApcmkC-ILNu"
      },
      "source": [
        "criterion_GAN = torch.nn.BCEWithLogitsLoss().cuda() # 저절로 sigmoid를 해주기 때문에 discriminator에 sigmoid불필요!\n",
        "criterion_content = torch.nn.MSELoss().cuda() # perceptual loss\n",
        "criterion_pixel = torch.nn.L1Loss().cuda()\n",
        "\n",
        "optimizer_G = torch.optim.Adam(netG.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(netD.parameters(), lr=lr, betas=(b1, b2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFP-4ZPUZdf3"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rKyMzJrZhH5"
      },
      "source": [
        "os.makedirs('/content/drive/MyDrive/AI604_TeamProject/improved_srgan_data/results/', exist_ok=True)\n",
        "os.makedirs('/content/drive/MyDrive/AI604_TeamProject/improved_srgan_data/checkpoints/', exist_ok=True)\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
        "\n",
        "# Retrain - load check points :\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/AI604_TeamProject/improved_srgan_data/checkpoints/no_noise_train_188.pth\") # no noise인지 noise포함 인지 확인 필수!******************************************\n",
        "# checkpoint = torch.load(\"/content/drive/MyDrive/AI604_TeamProject/improved_srgan_data/big_checkpoints/train_184.pth\")\n",
        "\n",
        "netG.load_state_dict(checkpoint['model_G_state_dict'])\n",
        "netD.load_state_dict(checkpoint['model_D_state_dict'])\n",
        "optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
        "optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
        "start = checkpoint['epoch']\n",
        "loss_content = checkpoint['loss_content']\n",
        "loss_pixel = checkpoint['loss_pixel']\n",
        "loss_G_real = checkpoint['loss_G_real']\n",
        "loss_G_fake = checkpoint['loss_G_fake']\n",
        "loss_GAN = checkpoint['loss_GAN']\n",
        "loss_G = checkpoint['loss_G']\n",
        "loss_real = checkpoint['loss_real']\n",
        "loss_fake = checkpoint['loss_fake']\n",
        "loss_D = checkpoint['loss_D']\n",
        "\n",
        "pix = len(train_dataloader)*0.25\n",
        "netG.train()\n",
        "netD.train()\n",
        "\n",
        "for epoch in range(start, n_epochs):\n",
        "# for epoch in range(n_epochs):\n",
        "  for batch_idx, imgs in enumerate(train_dataloader):\n",
        "\n",
        "    batches_done = epoch * len(train_dataloader) + batch_idx\n",
        "\n",
        "    # Configure model input\n",
        "    imgs_lr = Variable(imgs[\"lr\"].type(Tensor)).cuda()# 낮은 화질의 이미지\n",
        "    imgs_hr = Variable(imgs[\"hr\"].type(Tensor)).cuda() # 원래 이미지\n",
        "\n",
        "    noise = Variable(imgs[\"hr\"].type(Tensor).normal_(0, 0.1)) #gaussian noise\n",
        "\n",
        "    valid = Variable(Tensor(np.ones((imgs_lr.size(0), *netD.output_shape))), requires_grad=False)\n",
        "    fake = Variable(Tensor(np.zeros((imgs_lr.size(0), *netD.output_shape))), requires_grad=False)\n",
        "        \n",
        "    ########################### Train Generator ################################\n",
        "    optimizer_G.zero_grad()\n",
        "\n",
        "    # Generate a high resolution image from low resolution input\n",
        "    gen_hr = netG(imgs_lr)\n",
        "\n",
        "    # content loss\n",
        "    loss_pixel = criterion_pixel(gen_hr, imgs_hr)\n",
        "\n",
        "    # print(batches_done)\n",
        "    \n",
        "    if batches_done < pix:\n",
        "      Warm-up (pixel-wise loss only)\n",
        "      loss_pixel.backward()\n",
        "      optimizer_G.step()\n",
        "      continue\n",
        "\n",
        "    # Adversarial loss\n",
        "    pred_G_real = netD(imgs_hr).detach()\n",
        "    pred_G_fake = netD(gen_hr) # super resolution\n",
        "\n",
        "    loss_G_real = criterion_GAN(pred_G_real - pred_G_fake.mean(0, keepdim=True), fake) ######### ESRGAN에서 소개된 relativistic gan방법\n",
        "    loss_G_fake = criterion_GAN(pred_G_fake - pred_G_real.mean(0, keepdim=True), valid)\n",
        "\n",
        "    loss_GAN = (loss_G_real + loss_G_fake) / 2\n",
        "\n",
        "    # Perceptual loss - 생성된 이미지 밝기를 일정하게 해줌\n",
        "    gen_features = feature_extractor(gen_hr)\n",
        "    real_features = feature_extractor(imgs_hr).detach()\n",
        "    loss_content = criterion_content(gen_features, real_features)\n",
        "\n",
        "    # Total Generator loss\n",
        "    # 기존 GAN loss와 다르게 generator로 부터 생성한 이미지를 HR 이미지로 구별할 확률을 정해줍니다.\n",
        "    # 아래의 식으로 최소화하면 결과가 더 좋다고 합니다..\n",
        "    loss_G = loss_content + 0.005 * loss_GAN + 0.01 * loss_pixel\n",
        "\n",
        "    loss_G.backward()\n",
        "    optimizer_G.step()\n",
        "\n",
        "    ########################### Train Discriminator ############################\n",
        "    optimizer_D.zero_grad()\n",
        "    # Loss of real and fake images\n",
        "    # pred_real = netD(imgs_hr+noise) # add gaussian noise to discriminator to prevent discriminator overfitting*****************************************************************************\n",
        "    pred_real = netD(imgs_hr) # 150부터 noise 없이 해봄\n",
        "    pred_fake = netD(gen_hr).detach()\n",
        "\n",
        "    loss_real = criterion_GAN(pred_real - pred_fake.mean(0, keepdim=True), valid) ######### ESRGAN에서 소개된 relativistic gan방법\n",
        "    loss_fake = criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), fake)\n",
        "\n",
        "    # Total loss\n",
        "    loss_D = (loss_real + loss_fake) / 2\n",
        "\n",
        "    loss_D.backward()\n",
        "    optimizer_D.step()\n",
        "\n",
        "    ############################################################################\n",
        "\n",
        "    # Save Checkpoints\n",
        "    if (batch_idx+1 == len(train_dataloader)) and (epoch+1) % 1 == 0:\n",
        "      # print(\"chedking\")\n",
        "      torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_G_state_dict': netG.state_dict(),\n",
        "            'model_D_state_dict': netD.state_dict(),\n",
        "            'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
        "            'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
        "            'loss_content': loss_content,\n",
        "            'loss_GAN': loss_GAN,\n",
        "            'loss_pixel': loss_pixel,\n",
        "            'loss_G_real': loss_G_real,\n",
        "            'loss_G_fake': loss_G_fake,\n",
        "            'loss_G': loss_G,\n",
        "            'loss_real': loss_real,\n",
        "            'loss_fake': loss_fake,\n",
        "            'loss_D':loss_D,\n",
        "            # }, os.path.join('/content/drive/MyDrive/AI604_TeamProject/improved_srgan_data/checkpoints/', 'train_{:d}.pth'.format(epoch)))**********************************************************\n",
        "\n",
        "            }, os.path.join('/content/drive/MyDrive/AI604_TeamProject/improved_srgan_data/checkpoints/', 'no_noise_train_{:d}.pth'.format(epoch)))\n",
        "\n",
        "    if (batch_idx + 1) % 400 == 0:\n",
        "      print('Epoch [%d/%d], Step[%d/%d], lossD: %.4f, lossG: %.4f'\n",
        "      % (epoch+1, n_epochs, batch_idx+1, len(train_dataloader), loss_D.item(), loss_G.item()))\n",
        "    \n",
        "    # Save images\n",
        "    if batch_idx+1 == len(train_dataloader) and (epoch+1) %1 == 0:\n",
        "      imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\n",
        "      img_grid = denormalize(torch.cat((imgs_hr, imgs_lr, gen_hr), -1))\n",
        "      # 한 이미지에 원래 high, low, 만든 super가 들어갑니다!\n",
        "      # save_image(img_grid, os.path.join('/content/drive/MyDrive/AI604_TeamProject/improved_srgan_data/results/', 'train_fake-{:03d}.jpg'.format(epoch +1)), nrow = 1, normalize = False)***********************************\n",
        "      save_image(img_grid, os.path.join('/content/drive/MyDrive/AI604_TeamProject/improved_srgan_data/results/', 'no_noise_train_fake-{:03d}.jpg'.format(epoch +1)), nrow = 1, normalize = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3UfTL0rpkkV"
      },
      "source": [
        "## TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TaPjE7HWPkt"
      },
      "source": [
        "os.makedirs('/content/drive/MyDrive/AI604_TeamProject/evaluation_samples/for sfm/esrgan/3-1/office/', exist_ok=True) # Testing==========================\n",
        "\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
        "\n",
        "# Test - load check points :\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/AI604_TeamProject/improved_srgan_data/checkpoints/no_noise_train_199.pth\")\n",
        "\n",
        "netG.load_state_dict(checkpoint['model_G_state_dict'])\n",
        "\n",
        "netG.eval()\n",
        "\n",
        "# test_path = '/content/drive/MyDrive/AI604_TeamProject/final_train_data/168219_71.jpg'\n",
        "test_path = '/content/drive/MyDrive/AI604_TeamProject/evaluation_samples/synsin(256*256)/office'\n",
        "\n",
        "class TestImageDataset(Dataset):\n",
        "  def __init__(self, files):\n",
        "    self.transform = transforms.Compose(\n",
        "        [\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize(mean, std),\n",
        "        ]\n",
        "    )\n",
        "    self.files = files\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    img = Image.open(self.files[index % len(self.files)])\n",
        "    img = self.transform(img)\n",
        "    return img\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.files)\n",
        "\n",
        "# Prepare input\n",
        "# final_test_dataloader = DataLoader(TestImageDataset(glob.glob(test_path )), batch_size=1, shuffle=False, num_workers=0)\n",
        "final_test_dataloader = DataLoader(TestImageDataset(glob.glob(test_path + \"/*.*\")), batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "for i, img in enumerate(final_test_dataloader):\n",
        "\n",
        "  test_image = Variable(img.type(Tensor))\n",
        "\n",
        "  # Upsample image\n",
        "  with torch.no_grad():\n",
        "    sr_image = denormalize(netG(test_image)).cpu()\n",
        "\n",
        "  save_image(sr_image, os.path.join('/content/drive/MyDrive/AI604_TeamProject/evaluation_samples/for sfm/esrgan/3-1/office/', 'synsin_office{:d}.jpg'.format(i+1)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S84xBdQEpb0h"
      },
      "source": [
        "# **Test** Image low resolution으로 바꾸기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLAUMMj_mKpu"
      },
      "source": [
        "# # make low resolution\n",
        "# hy_test_path = \"/content/drive/MyDrive/AI604_TeamProject/evaluation_samples/origin/장하영\"\n",
        "# hr_shape = (512, 512)\n",
        "\n",
        "# test_dataloader = DataLoader(ImageDataset(glob.glob(hy_test_path + \"/*.*\"), hr_shape=hr_shape), batch_size=1, shuffle=False, num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxqbhAeVngnR"
      },
      "source": [
        "# os.makedirs('/content/drive/MyDrive/AI604_TeamProject/improved_srgan_data/test_results/low_resolution/', exist_ok=True)\n",
        "# Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
        "\n",
        "# for i, imgs in enumerate(test_dataloader):\n",
        "#     imgs_lr = Variable(imgs[\"lr\"].type(Tensor)).cuda()# 낮은 화질의 이미지\n",
        "#     imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\n",
        "#     imgs_lr = denormalize(imgs_lr)\n",
        "#     save_image(imgs_lr, os.path.join('/content/drive/MyDrive/AI604_TeamProject/improved_srgan_data/test_results/low_resolution/', 'hy_lr-{:d}.jpg'.format(i +1)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}